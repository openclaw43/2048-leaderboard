name: Benchmark

on:
  pull_request:
    types: [opened, synchronize]
    paths:
      - 'game2048/**/*.py'
      - 'benchmark.py'
      - 'pyproject.toml'
      - '.github/workflows/benchmark.yml'
  workflow_dispatch:

permissions:
  pull-requests: write

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      
      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: 'latest'
      
      - name: Install dependencies
        run: uv sync
      
      - name: Run benchmarks
        run: uv run python benchmark.py --parallel --json --output benchmark_results.json
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark_results.json
      
      - name: Comment on PR
        if: github.event_name == 'pull_request'
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          python3 << 'EOF'
          import json
          import subprocess
          
          with open('benchmark_results.json') as f:
              results = json.load(f)
          
          summary = results.get('summary', {})
          confidence_intervals = results.get('confidence_intervals', {})
          rank_ranges = results.get('rank_ranges', {})
          
          if not summary:
              print("No summary data available")
              exit(0)
          
          sorted_agents = sorted(summary.items(), key=lambda x: x[1]['avg_score'], reverse=True)
          
          comment = "## Benchmark Results (with 95% confidence intervals)\n\n"
          comment += "| Rank | Agent | Avg Score | 95% CI | Max Score | Avg Max Tile |\n"
          comment += "|------|-------|-----------|--------|-----------|-------------|\n"
          
          for i, (name, stats) in enumerate(sorted_agents, 1):
              rank_info = rank_ranges.get(name, {})
              rank_display = rank_info.get('display', f'#{i}')
              ci_info = confidence_intervals.get(name, {})
              ci_lower = ci_info.get('lower', stats['avg_score'])
              ci_upper = ci_info.get('upper', stats['avg_score'])
              ci_display = f"[{ci_lower:.0f}, {ci_upper:.0f}]"
              comment += f"| {rank_display} | **{name}** | {stats['avg_score']:.1f} | {ci_display} | {stats['max_score']} | {stats['avg_max_tile']:.1f} |\n"
          
          alpha = results.get('statistical_params', {}).get('alpha', 0.05)
          comment += f"\n_Statistical significance level: Î± = {alpha}_\n"
          comment += f"_Timestamp: {results.get('timestamp', 'N/A')}_"
          
          with open('pr_comment.md', 'w') as f:
              f.write(comment)
          EOF
          
          if [[ -f pr_comment.md ]]; then
            gh pr comment ${{ github.event.pull_request.number }} --body-file pr_comment.md
          fi
